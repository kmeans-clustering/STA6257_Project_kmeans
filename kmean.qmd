---
title: "K-Means Clustering"
subtitle: <div style="color:#ffbf00; margin-bottom:40px">An unsupervised learning algorithm</div>
# title-slide-attributes:
#   data-background-image: kmeans.png
#   data-background-size: contain
#   data-background-opacity: "0.5"
author:
  - <div style="color:#13f4ef">Anand Pandey</div>
  - <div style="color:#13f4ef">Katie Hidden</div>
  - <div style="color:#13f4ef">Akash Chandra</div>
format: 
  revealjs:
    slide-number: true
    chalkboard: true
    preview-links: auto
    transition: slide
    background-transition: fade
    theme: styles.scss
    footer: <http://kmean.scrib.ink/>
---

## Overview {background-image="intro.png" background-size="476px 512px" background-position="bottom 50px right 100px"}

-   What is _K-Means Clustering_?

-   Why and where would we need _K-Means Clustering_?

-   How can we create _K-Means Clusters_?

-   What is our business usecase?

-   What business value can we extract from the clusters?

-   Why K-Means clustering isn’t (always) the answer?

-   How can we do it better?

## K-Means Clustering

::: columns
::: {.column width="30%"}
![~*<span style="font-size:12px">Image Source:©Anand</span>*~](cIDB3.png)
:::

::: {.column .r-fit-text width="70%"}
::: {style="font-size:28px"}
<br/>
An unsupervised machine learning clustering algorithm

-   Data is clustered based on feature similarity

-   Unsupervised: Data is unlabeled, groups are unknown

-   Find similar groups, glean insights

-   Dataset may be very large and highly dimensional
:::
:::
:::

## Why would we need Clustering?

::: columns
::: {.column width="50%"}
![~*<span style="font-size:12px">Image Source:©Anand</span>*~](uses.png)
:::

::: {.column .r-fit-text width="50%"}
::: {style="font-size:28px"}
<br/>
**Main usecases**:

-   A large unstructured dataset is to be clustered<br/> without any instructions

-   No prior information on how many groups we<br/> might need to divide our data into
:::
:::
:::

##    How can we create Clusters?

::: {.panel-tabset}

### Algorithm

::: columns
::: {.column width="50%"}
![](kmean_s0.png)
:::

::: {.column .r-fit-text width="50%"}
::: {style="font-size:28px;margin-left:60px"}
![](flowchart.png)

:::
:::
:::

###   The algorithm in action

::: columns
::: {.column width="60%"}
![*<span style="font-size:12px">Image Source: http://shabal.in/</span>*](random.gif)
:::

::: {.column width="40%"}
![](flowchart.png)
:::
:::

###   The Math

The Math behind K-Means Algorithm

-   K-Means runs in an unsupervised environment and hence it measures the quality of the formed clusters by finding the variations within each cluster.


::: {.callout-note icon=false}
##    The goal will be to minimize this intra-cluster variation (also called WCSS)
$$
WCSS = \sum_{i = 1}^{k}\sum_{x ∈ C_i}|x-μ_i|^2
$$
:::
Where, K is the # of disjoint cluster $C_i$, x is a data point in the cluster $C_i$, $μ_i$ is the mean of the data points in the cluster $C_i$.

### Distance Measures



::: columns
::: {.column width="50%"}
![~*<span style="font-size:12px">Image Source:©Anand</span>*~](dist.png)
:::

::: {.column .r-fit-text width="50%"}

<div style="font-size: 20px">
-   K-means calculates proximity between data points<br/> and centroids to place the data in appropriate cluster.

-   Euclidean distance

$$
D_{euc}(x, y) = \sqrt{\sum_{i = 1}^{n}{(x_i - y_i)^2}}
$$

-   Manhattan distance
    
$$
D_{man}(x, y) = \sum_{i = 1}^{n}|(x_i - y_i)|
$$

</div>
:::
:::


:::


##    Our Business usecase
###   `Customer Segmentation`

::: columns
::: {.column width="50%"}
![~*<span style="font-size:12px">Image Source:©Anand</span>*~](slide1.png)
:::

::: {.column .r-fit-text width="50%"}
-   Dividing customers into number of focused groups that are as dissimilar as possible across groups, but as similar as possible within each group in specific ways with shared buying characteristics that is relevant to marketing

-   The chosen attributes will play a key role in deciding the groups.

-   Algorithm is based around analyzing what we call RFM - customer recency, frequency, and monetary values
:::
:::


##    Our Business usecase
###   `The Dataset`
```{r, warning=FALSE, echo=FALSE, message=FALSE}
library(readxl)
library(papeR)
library(dplyr)
library(tidyverse)
library(kableExtra)
library(cowplot)
```
```{r}
#| warning: false
#| echo: false
filename <- "online_retail_II.xlsx"
df = read_excel(filename)
 
names(df)[names(df) == 'Customer ID'] <- 'CustomerID'
```

The data set contains the transactions from a UK-based online retail store between 01/12/2009 and 09/12/2011.

::: {.panel-tabset}

###   Data Definition
<div style="font-size:14px">
| Attribute | Type | Description
|:----------|:-----|:-----------|
| Invoice   | Nominal   | A 6-digit unique transaction #. If starts with `c`, indicates a cancellation.|
| CustomerID| Numeric | A 5-digit unique customer Id. |
| StockCode | Nominal | A 5-digit code uniquely assigned to each distinct product. |
| Quantity     | Numeric  |   The quantities of each product (item) per transaction. |
| InvoiceDate       | Numeric   |     The day and time when a transaction was generated. |
| Price | Numeric | Product price per unit in sterling (£). |
| Country | Nominal | The name of the country where a customer resides. |
| Description | Nominal | Product name. |
</div>

###   Data Preview
<!-- ![~*<span style="font-size:12px">Image Source:©Anand</span>*~](datapreview.png) -->

```{r}
#| warning: false
#| echo: false
head(df) %>%
  kbl() %>%
  kable_paper("hover", full_width = T)
```

###   Initial Observations

-   There are 525461 records in the dataset

-   107927 Customer Ids are missing (only 417534 customers are identifiable)


:::

##    Our Business usecase
###   `Data Visualization`

::: {.panel-tabset}
###   Visualization
```{r}
#| warning: false
#| echo: false
df1=na.omit(df, cols="CustomerID")
df1$QuantityRange <- cut(df1$Quantity, c(-Inf, 1, 5, 10, 20, 30, 50, Inf))
g1 <- ggplot(df1,aes(QuantityRange))+geom_histogram(stat= "count", aes(fill = ..count..))+ labs(title="Quantity Histogram")+xlab('Quantity')+ylab('Count')+ theme(axis.text.y=element_blank(), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
  #theme(axis.text.x=element_blank(),axis.text.y=element_blank())
#g1
df1$PriceRange <- cut(df1$Price, c(-Inf, 1, 2, 4, 6, 8, 10, 50, Inf))
g2 <- ggplot(df1,aes(PriceRange))+geom_histogram(stat= "count", aes(fill = ..count..))+ labs(title="Price Histogram")+xlab('Price')+ylab('Count')+ theme(axis.text.y=element_blank(), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
  #theme(axis.text.x=element_blank(),axis.text.y=element_blank())
#g2
 
g3 <- df1 %>%
    group_by(StockCode) %>%
    summarise(count = n()) %>%
    top_n(n = 5, wt = count)  %>% ggplot(aes(x=reorder(StockCode, -count), y=count, fill=StockCode))+geom_bar(stat= "identity")+ labs(title="Top 5 StockCode")+xlab('StockCode')+ylab('Count')+  theme(axis.text.x=element_blank(),axis.text.y=element_blank())
 
#g3
 
g4 <- df1 %>%
    group_by(Country) %>%
    summarise(count = n()) %>%
    top_n(n = 5, wt = count) %>%
    ggplot(aes(x=reorder(Country, -count), y=count, fill=Country))+geom_bar(stat="identity")+ labs(title="Top 5 Country")+xlab('Country')+ylab('Count')+ theme(axis.text.x=element_blank(),axis.text.y=element_blank())
#g4
 
plot_grid(g1, g2, g3, g4,
          labels = c("A", "B", "C", "D"),
          ncol = 2, nrow = 2)
```
###   Observations
-   The Quantity is heavily skewed right and most of the transactions involved a quantity in the range of 1 to 5

-   The unit Price is also skewed right and most of the items have unit price between 1 and 4

-   The item with StockCode 85123A was sold most

-   Most of the customers belong to United Kingdom
:::

##    Our Business usecase
###   `Data Preparation`

::: {.panel-tabset}

###   Cleanse data    

::: columns
::: {.column .r-fit-text width="50%"}

::: {style="font-size:18px; line-height:1.5"}

1.    Remove missing values

```{.r}
df=na.omit(df, cols="CustomerID")
```
<br/>
2.    Select only required attributes - CustomerID,<br/>Invoice, Quantity, InvoiceDate and Price

```{.r}
data = subset(df, select = -c(Description, Country, StockCode) )
```
<br/>
3.    Create a new attribute Amount

```{.r code-line-numbers="1"}
data$Amount = data$Quantity * data$Price
#Group by customer id and add amounts
customer_monetary <- as.data.frame(data %>%
  group_by(CustomerID) %>%
  summarise(Amount = sum(Amount)))
```


:::
:::

::: {.column .r-fit-text width="50%"}
::: {style="font-size:18px"}

4.    Group by customer id and count invoices

```{.r code-line-numbers="1"}
customer_frequency <- as.data.frame(data %>% group_by(CustomerID) 
%>% summarise(Invoice = n()))
```

5.    Create a new attribute LastSeen

```{.r code-line-numbers="1-2"}
data$LastSeen =as.integer(difftime(max(data$InvoiceDate), 
  data$InvoiceDate, units = "days"))
#Group by customer id and take min of LastSeen
customer_lastseen <-  as.data.frame(data %>% group_by(CustomerID) 
%>% summarise(LastSeen = min(LastSeen)))
```

6.    Merge the above data frames to get unique customer and their total amount,<br/> frequency of visit to the store and the LastSeen shopping in the store

```{.r}
customer <- merge(customer_monetary, customer_frequency, by = 'CustomerID')
customer <- merge(customer, customer_lastseen, by = 'CustomerID')
```

:::
:::
:::

### Detect outliers
<div style="text-align:center">
::: {layout="[[1],[1,1]]"}

![](outliers_r.png){fig-align="center"}

![](outlier1.png)

![](outlier2.png)
:::
</div>

###   Code

```{.r code-line-numbers="2-11"}
# create detect outlier function
detect_outlier <- function(x) {
    # calculate first quantile
    Quantile1 <- quantile(x, probs=.25)
    # calculate third quantile
    Quantile3 <- quantile(x, probs=.75)
    # calculate inter quartile range
    IQR = Quantile3-Quantile1
    # return true or false
    x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
# create remove outlier function
remove_outlier <- function(dataframe,
                            columns=names(dataframe)) {
    # for loop to traverse in columns vector
    for (col in columns) {
        # remove observation if it satisfies outlier function
        dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
    }
    return(dataframe)
}

# detect_outlier(customer$Amount)
customer <- remove_outlier(customer, c('Amount','Frequency','LastSeen'))
```

:::

##    Our Business usecase
###   `Target Dataset`
<div style="font-size:20px">
```{r}
#| warning: false
#| echo: false
#| results: asis

a <- data.frame(vars = c('CustomerID', 'Amount', 'Frequency', 'LastSeen'),               
                DataType = c('Numeric', 'Numeric', 'Numeric', 'Numeric'),
                Description = c('Customer number. A 5-digit integral number uniquely assigned to each customer.',
    'Total Amount spent by each unique customer',
    'Number of times customer made purchases bases on their invoice.',
    'Days elapsed since the customer was last seen shopping.'),
                stringsAsFactors = FALSE)
 
kable(a,
      caption = "Target Data Definition",
      col.names = c("Attribute", "Type", "Description"),
      align="lcl", escape = FALSE) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = TRUE) %>%
  row_spec(0, bold = T, background = "#666666", color = "white")

```
<br/>
```{r}
#| warning: false
#| echo: false
#| results: asis
a <- data.frame(vars = c('Amount', 'Frequency', 'LastSeen'),                       
                Mean = c('776.5', '48.15', '101.8'),             
                Min = c('-1598.7','1','0'),
                Q1 = c('241.5', '15.0', '23.0'),
                Median = c('515.1', '34.0', '62.0'),
                Q3 = c('1091.4', '70.0', '163.0'),
                Max = c('3685.1', '183.0','373.0'),
                stringsAsFactors = FALSE)
 
kable(a,
      caption = "Summary of Target Dataset (3674 Records)",
      col.names = c("", "Mean","Min", "Q1", "Median", "Q3", "Max"),
      align="lcc", escape = FALSE) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = TRUE) %>%
  row_spec(0, bold = T, background = "#666666", color = "white")
```
</div>

##    Our Business usecase
###   Statistical Modeling: `Optimal # of Clusters`

::: {style="font-size:24px"}
The reliability and the performance of a clustering algorithm is directly affected by the initial choice of the number of clusters (K)
:::
::: {.panel-tabset}

### Elbow Method

::: columns
::: {.column width="50%"}
<br/>

```{.r code-line-numbers="1"}
fviz_nbclust(customer, kmeans, method = "wss") 
  + geom_vline(xintercept = 3, linetype = 2)
  + labs(subtitle = "Elbow Method")
```
<br/>
-   Calculates values of cost with changing K

-   The larger number of clusters implies the data points are closer to the centroid

-   The point where this distortion declines the most is the elbow point
:::

::: {.column .r-fit-text width="50%"}
::: {style="font-size:28px;margin-left:60px"}
![](elbow.png)

:::
:::
:::


### Silhouette Method


::: columns
::: {.column width="50%"}
<br/>

```{.r code-line-numbers="1"}
fviz_nbclust(customer, kmeans, method = "silhouette")
  + theme_classic()
  + labs(subtitle = "Silhouette Method")
```
<br/>
-   Calculates a coefficient which is a measure of how similar a data point is within-cluster (cohesion) compared to other clusters (separation)

:::

::: {.column .r-fit-text width="50%"}
::: {style="font-size:28px;margin-left:60px"}
![](silhouette.png)

:::
:::
:::


### Gap Statistic Method


::: columns
::: {.column width="50%"}
<br/>

```{.r}
gap_stat <- clusGap(customer, FUN=kmeans, nstart=25,
              K.max=10, B=50)
fviz_gap_stat(gap_stat)
```
<br/>
-   Gap Statistics relies on an approach where the number K is chosen based on the biggest fluctuation in the within-cluster distance
:::

::: {.column .r-fit-text width="50%"}
::: {style="font-size:28px;margin-left:60px"}
![](gap.png)

:::
:::
:::

:::


##    Our Business usecase
###   Statistical Modeling: `The K-Means Model`
<br/>
K-Means model with K = 3

```{.r code-line-numbers="4"}
# Compute k-means with k = 3
set.seed(123)
k = 3
m1.kmean <- customer_scaled %>% kmeans(k, iter.max = 20, nstart = 25)
#nstart: Runs multiple initial configurations and returns the best one.
 
#Add Clusters to the original dataframe
customer$Cluster <- as.factor(m1.kmean$cluster)
```

The model results in 3 clusters as was decided and the clusters are formed with sizes 1807, 953, 914.


##   The Business Value?
###    `Results and Analysis`

::: {.panel-tabset}

###   2-D Analysis


<div style="text-align:center;">
::: {layout="[[1,1,1]]"}
![](res1.png)

![](res2.png)

![](res3.png)
:::
</div>
<div style="margin-top:-20px; font-size:20px">
-   The clusters 1 and 3 are seen in the store often. Cluster 3 customers are spending more whenever they visit. Cluster 2 not seen recently so spend less

-   The Cluster 3 are more frequently buying and hence spending more. Cluster 1 and 2 are almost having similar trends in terms of number of purchases and amount spent

-   The Cluster 1 and 3 are seen buying often and whenever they are seen buying, they make lot of purchases
</div>
### 3-D Analysis


::: columns
::: {.column width="50%"}


![](res4.png)


:::

::: {.column .r-fit-text width="50%"}
::: {style="font-size:20px;margin-left:20px"}

-   The customers in cluster 3 are seen shopping often, they spend more,<br/>and they purchase frequently

-   Customers in cluster 2 are of concern. They do not visit the website<br/>often but spend almost like cluster 1 and purchase as frequently as cluster 1. 

So, we can either target cluster 1 to make them buy when they visit store<br/>(since they are visiting often but buying less) or make cluster 2 visit often<br/>as they are buying more when they visit but visiting less.
:::
:::
:::

### Box Plots

<div style="text-align:center">
::: {layout="[[1,1,1]]"}
![](res5.png)

![](res6.png)

![](res7.png)
:::
</div>
<div style="margin-top:-20px; font-size:20px">
-   Customers in Cluster# 3 are spending more whereas cluster 2 customers are spending the least

-   Customers in Cluster# 3 are most frequent customers indicating that the frequent customers spend the most

-   Customers in Cluster 2 have not been seen shopping recently. The most recently seen shoppers are in Cluster 3 and that also explains why they have spent more money and bought frequently
</div>
###   Results

![](RFM.png)
:::

##    Why clustering isn’t (always) the answer? 
###   `Limitations`

::: {.panel-tabset}

###   K-Value

-   Challenging for large and highly dimensional datasets

    -   Difficult to visualize data
    
-   Developed in the 1960s

    -   Before advent of the internet and "big data"
    
    -   Not designed for datasets of such large scale
    
###   Convergence at Local Minimum

Convergence at Local Minimum/Local Optimum Solution

-   The algorithm is sensitive to:

    -   Initial centroid value selection
    
    -   Noise
    
    -   Outliers
    
-   Clusters veer off and converge at local minimum rather than true global minimum

###   Shape of clusters

Shape of Data Clusters

-   The algorithm classifies n-hyperspherically shaped data well...

-   Less successful at classifying irregularly shaped clusters

:::

##    How can we do it better?
###   `Improvements`

Researchers have developed many variations of the K-means algorithm to overcome limitations.

::: {.panel-tabset}

###   U-k-means

A "novel unsupervised k-means (U-k-means) clustering algorithm"

-   Created to overcome the challenge of selecting a K-value for very large datasets

-   No parameter initialization required

-   Built-in process determines K-value and performs K-means clustering using the concept of entropy

    -   The K-value is initialized as the number of data points
    
    -   Extra clusters are discarded until the "best" number of clusters is found

###   CLustering In QUEst (CLIQUE)

Method to determine K-value and detect and remove outliers and noise.

-   Regional density is calculated using a CLIQUE grid. Each dimension is divided into equal parts

-   The number of data points is calculated within each cell

    -   Cells with counts above a defined threshold are defined as **dense units**. Adjacent dense units are connected to determine regional density
    
    -   Noise in areas of low density is removed

###   CFSFDP

Clustering by Fast Search and Find of Density Peaks. Method to approximate better initial cluster centroids.

-   Used in conjunction with CLIQUE

-   Local density of each point is calculated to determine optimum initial cluster centers

-   CLIQUE combined with CFSFDP can handle irregularly shaped clusters

:::

##   Conclusion
<br/>
<div style="line-height:2">
-   K-Means clustering is a popular unsupervised learning algorithm

-   K-Means can be a great tool to achieve the segmentation of customers with speed and accuracy

-   K-Means is not always accurate and has some limitations

-   There are some improved algorithms that can help overcome these shortcomings.
</div>
##    


<div style="text-align:center; font-size: 48px; margin-top: 300px">
Thank You!
</div>
